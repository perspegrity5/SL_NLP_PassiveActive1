{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebook space for Passive Active NLP StageLens project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General module loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.3\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz (852.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 852.3MB 15.4MB/s ta 0:00:01  1% |▍                               | 11.6MB 5.5MB/s eta 0:02:32    2% |▊                               | 19.9MB 41.6MB/s eta 0:00:20    5% |█▉                              | 49.9MB 42.1MB/s eta 0:00:20    9% |███                             | 81.3MB 39.4MB/s eta 0:00:20    9% |███                             | 82.1MB 19.3MB/s eta 0:00:40    10% |███▎                            | 86.5MB 25.4MB/s eta 0:00:31    14% |████▋                           | 121.4MB 20.5MB/s eta 0:00:36    16% |█████▎                          | 141.0MB 10.6MB/s eta 0:01:08    17% |█████▋                          | 149.8MB 11.4MB/s eta 0:01:02    24% |███████▊                        | 204.6MB 9.5MB/s eta 0:01:09    25% |████████▏                       | 216.5MB 9.4MB/s eta 0:01:08    26% |████████▍                       | 223.0MB 10.6MB/s eta 0:01:00    27% |█████████                       | 238.2MB 11.6MB/s eta 0:00:54    35% |███████████▍                    | 302.5MB 9.5MB/s eta 0:00:58    35% |███████████▍                    | 303.6MB 9.1MB/s eta 0:01:01    35% |███████████▌                    | 305.4MB 8.9MB/s eta 0:01:02    41% |█████████████▎                  | 353.4MB 13.2MB/s eta 0:00:38    42% |█████████████▌                  | 359.7MB 9.7MB/s eta 0:00:51    51% |████████████████▌               | 440.2MB 11.6MB/s eta 0:00:36    52% |████████████████▋               | 443.9MB 13.4MB/s eta 0:00:31    59% |███████████████████             | 506.5MB 11.2MB/s eta 0:00:31    63% |████████████████████▍           | 542.6MB 12.5MB/s eta 0:00:25    65% |█████████████████████           | 560.1MB 14.7MB/s eta 0:00:20    72% |███████████████████████         | 614.0MB 14.4MB/s eta 0:00:17    75% |████████████████████████▎       | 647.7MB 15.6MB/s eta 0:00:14\n",
      "\u001b[?25h  Requirement already satisfied (use --upgrade to upgrade): en-core-web-lg==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz in /srv/venv/lib/python3.6/site-packages\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /srv/venv/lib/python3.6/site-packages/en_core_web_lg -->\n",
      "    /srv/venv/lib/python3.6/site-packages/spacy/data/en_core_web_lg\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_lg')\n",
      "\n",
      "Downloaded\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "!python -m spacy download en_core_web_lg\n",
    "\n",
    "print(\"Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded models\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import html\n",
    "from spacy import displacy\n",
    "import re\n",
    "PATTERN = \"[^a-zA-Z0-9_\\s'\\.]+\"\n",
    "rgx = re.compile(PATTERN, re.IGNORECASE)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(\"Loaded models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv = \"\"\"Sometimes I wish that\tmy mom would come home more often.\t|1| my mom would come home more often.\n",
    "My family\thas a cat...I don't have a cat though I have 2 dogs\t|1| has a cat...|2| I don't have a cat |3| though I have 2 dogs\n",
    "When I am nervous\tmy hands get shaky\t|1| my hands get shaky\n",
    "At times I worry about\tmy own dreams, nightmares I have thousands of them\t|1| my own dreams, nightmares |2| I have thousands of them\n",
    "Technology\tit's crazy and i wish i could keep up\t|1| its crazy and |2| i wish |3| i could keep up\n",
    "When I get mad\tI sometimes use big voice then I go somewhere quiet so I can cry by myself.\t|1| I sometimes use big voice |2| then I go somewhere quiet |3| so I can cry by myself.\n",
    "My main problem is\ta feeling that I am not entitled\t|1| a feeling |2| that I am not entitled\n",
    "If my mother\twas a different lady, I would not be who I am.\t|1| was a different lady, |2| I would not be |3| who I am.\n",
    "When I get mad\tit takes a long time to build up, but I am slow to forgive.\t|1| it takes a long time to build up, |2| but I am slow to forgive.\n",
    "When they avoided me\tI felt safe, knowing that I could receive a warm welcome from my family and friends.\t|1| I felt safe, |2| knowing |3| that I could receive a warm welcome from my family and friends.\n",
    "Sometimes I wish that\tI could be younger and have done things differently.\t|1| I could be younger |2| and have done things differently.\n",
    "When they avoided me\tI sought to find out 'why?' . Knowing the 'why' can begin Awareness and Healing\t|1| I sought to find out 'why?' . |3| Knowing the 'why' |4| can begin Awareness and Healing\n",
    "Being with other people\tis both a learning experience and an opportunity to exchange views.\t|1| is both a learning experience and an opportunity |2| to exchange views.\n",
    "My main problem is\tonly a main problem if I think it is.\t|1| only a main problem |2| if I think |3| it is.\n",
    "When people are helpless\tAt times I try to find other ways of doing thing. But sometime I might be disappointed and stop or quit.\t|1| At times I try |2| to find other ways |3| of doing thing. |4| But sometime I might be disappointed |5| and stop |6| or quit.\n",
    "My father\ttaught me how to think critically and to love deeply.\t|1| taught me |2| how to think critically |3| and to love deeply.\n",
    "If I can\\'t get what I want\tI try to work out another way of getting what I want\t|1| I try |2| to work out another way |3| of getting |4| what I want\n",
    "When they avoided me\thmmm... I\\'m stumped. I have no idea how to respond to this... Although there is a sense of relief...\t|1| hmmm... I'm stumped. |2| I have no idea |3| how to respond to this...|4| Although there is a sense of relief...\n",
    "Loving other people\tcan deepen with familiarity. Even with those who aren't a natural fit the differences between us can add color to the world as well as an opportunity to turn inward to see what is being triggered. Having said that, it's difficult for me to open my heart and trust.\t|1| can deepen with familiarity. |2| Even with those who aren't a natural fit |3| the differences between us can add color to the world |4| as well as an opportunity to turn inward |5| to see |6| what is being triggered. |7| Having said that, |8| it's difficult for me |9| to open |10| my heart |11| and trust.\n",
    "When people are helpless\tI am inspired to reach out and be of service.\t|1| I am inspired |2| to reach out |3| and be of service.\n",
    "Rules\tare guard rails. It is good and helpful to have them. When you head in the right direction you never get in touch with them, no matter how fast or slow you travel. Good rules are not in your way.\t|1| are guard rails. |2| It is good and helpful |3| to have them. |4| When you head in the right direction |5| you never get in touch with them, |6| no matter how fast or slow you travel. |7| Good rules are not in |8| your way.\n",
    "Technology\tis a great tool, nice to have and can bring global humanity closer together in ways never before imagined. But if it were to disappear tomorrow, I'm of the generation that would be able to survive.\t|1| is a great tool, |2| nice to have |3| and can bring global humanity closer together |4| in ways never before imagined. |5| But if it were |6| to disappear tomorrow, |7| I'm of the generation |8| that would be able |9| to survive.\n",
    "A good boss\tA good boss is critical toward making a workplace environment healthy and successful!\t|1| A good boss is critical |2| toward making a workplace environment healthy and successful!\n",
    "My main problem is\tgetting over decades spent proving myself.\t|1| getting over |2| decades spent proving myself.\n",
    "I feel sorry\tfor the pain that all humans and all beings on this earth must endure as part of the experience of being here.\t|1| for the pain |2| that all humans and all beings on this earth must endure |3| as part of the experience of being here.\n",
    "If I were in charge\ti would change that by facilitating change that made everyone in charge ... to the degree of their capabilities in wordly matters, but always in charge of their interior and their right to decide for themselves what is real and true for them.\t|1| i would change that |2| by facilitating change |3| that made everyone in charge ...|4| to the degree of their capabilities in wordly matters,|5| but always in charge |6| of their interior and |7| their right |8| to decide for themselves |9| what is real and true for them.\n",
    "At times I worry about\tbig, big existential questions, and little microscopic ones ... sometimes within milliseconds of one another.\t|1| big, big existential questions, and little microscopic ones...|2| sometimes within milliseconds of one another.\n",
    "Raising a family\tis a profound gift. I am tending the future. I am tending the life processes. It is a place for love and life and evolution to soar.\t|1| is a profound gift. |2| I am tending the future. |3| I am tending the life processes. |4| It is a place for love and life and evolution to soar.\n",
    "At times I worry about\thow differently I see and process things from many of the people I encounter, and whether I am wisely acting and interacting.\t|1| how differently I see |2| and process things |3| from many of the people I encounter, |4| and whether I am wisely acting |5| and interacting.\n",
    "What I like to do best is\tbe a conduit. There is no I.\t|1| be a conduit. |2| There is no I.\n",
    "Being with other people\tcan be an isolating or a hugely enriching experience, the latter when each others presence liberates emergent thoughts or self expressions bringing forth a collective that is more than the sum of its parts\t|1| can be an isolating or a hugely enriching experience, |1| the latter when each others presence liberates emergent thoughts or self expressions |3| bringing forth a collective |4| that is more than the sum of its parts\n",
    "A teacher has the right to\tI don't mean to be oppositional. . but this sentence stem. . .like so many others. . isn't one I can really answer outside of context. Rights. This is a wobbly construct. I'm sorry. I'm out of time otherwise I would write a bit about the possibility of rights . .the idea of having a right. . the usage of the language of rights.\t|1| I don't mean |2| to be oppositional. .|3| but this sentence stem. . .like so many others. . isn't one |4| I can really answer outside of context.|5| Rights. This is a wobbly construct.|6| I'm sorry.|7| I'm out of time |8| otherwise I would write a bit about the possibility of rights ..the idea of having a right. . the usage of the language of rights.\n",
    "Being with other people\tis most often a pleasure and fascinating experience of seeing a completely different version of Self.\t|1| is most often a pleasure and fascinating experience |2| of seeing a completely different version of Self.\n",
    "The past\tis a construct to give form and structure to the present moment.\t|1| is a construct |2| to give form and structure to the present moment.\n",
    "We could make the world a better place if\tthe mystery was okay to be as a question, that is to say if the question was okay to be a question and the answer could be a question and the answers and questions could be held together because they need each other and we realize a little faster that they won't end and they are growing together and rest in that.\t|1| the mystery was okay |2| to be as a question, |3| that is to say |4| if the question was okay |5| to be a question |6| and the answer could be a question |7| and the answers and questions could be held together |8| because they need each other |9| and we realize a little faster |10| that they won't end |11| and they are growing together |12| and rest in that.\n",
    "A good boss\tis a solar being ... one that brings light and eternal love without demanding anything of the others ... an energetic and gravitational field of such calm, peaceful magnitude that all are warmed and captivated by their glow ... brings a new burst life to all those who are in their presence\t|1| is a solar being ...|2| one that brings light and eternal love |3| without demanding anything of the others ..|4| an energetic and gravitational field of such calm, peaceful magnitude |5| that all are warmed and captivated by their glow ... |6| brings a new burst life |7| to all those who are in their presence\n",
    "My conscience bothers me if\tI'm not placing my feet on the path with an open heart, mind and spirit, tuning in to what's moving and showing up fully at the dance.\t|1| I'm not placing |2| my feet on the path with an open heart, mind and spirit,|3| tuning in to |4| what's moving |5| and showing up fully at the dance.\n",
    "When I am criticized\tI can embody my personality and feel a pang contrast to the story I tell myself inside the story we tell ourselves of what we think in the moment to be an absolute before it falls away into a deeper, clearer calm of letting go.\t|1| I can embody |2| my personality |3| and feel a pang contrast |4| to the story I tell myself |5| inside the story we tell ourselves |6| of what we think in the moment |7| to be an absolute |8| before it falls away into a deeper, clearer calm |9| of letting go.\n",
    "The thing I like about myself is\tthat here I sit in the drop of this essential moment, chuckling at this question! :-D\t|1| that here I sit in the drop of this essential moment, |2| chuckling at this question! :-D\n",
    "My main problem is\tintegrating emptiness with everyday life, finding meaning in meaningless given the meaningless in meaning making.\t|1| integrating emptiness with everyday life, |2| finding meaning in meaningless |3| given the meaningless |4| in meaning making.\n",
    "My main problem is\thow my narccissim gets in the way of seeing the truth of the illusion of self.\t|1| how my narccissim |2| gets in the way |3| of seeing the truth of the illusion of self.\n",
    "These days, work\tIs a rainbow of consciousness manifesting through each sparkling emanation called a 'me'.\t|1| Is a rainbow of consciousness |2| manifesting through each sparkling emanation |3| called a 'me'.\n",
    "Being with other people\tIs a mutually mutating meeting of universes\t|1| Is a mutually mutating meeting of universes\n",
    "What I like to do best is\tlay back and live the window that I have become as if a home with no walls, the infinite moves through as if there is no solid boundary.the veil is no more and the whole field that is everywhere exists where the inside and outside have no difference.\t|1| lay back |2| and live the window |3| that I have become as if a home with no walls, |4| the infinite moves through |5| as if there is no solid boundary. |6| the veil is no more |7| and the whole field that is everywhere |8| exists where the inside and outside |9| have no difference.\n",
    "Change is\tthe heartbeat of the universe... aliveness calling us forth!\t|1| the heartbeat of the universe...|2| aliveness calling us forth!\n",
    "When I get mad\ti feel adrenaline through my human body and it is fierce and i love it because it moves me and it is no different than the Spirit which moves all.\t|1| i feel adrenaline through my human body |2| and it is fierce |3| and i love it |4| because it moves me |5| and it is no different |6| than the Spirit which moves all.\n",
    "Raising a family\tCaptivates my whole being, just as every window on reality sees it is held in Love beyond knowing.\t|1| Captivates my whole being, |2| just as every window on reality sees |3| it is held in Love beyond knowing.\n",
    "If I were in charge\tIs an expression of the love animating all creation, the simple joy that moves all from rock to life to mind to All. It comes through this glorious One in all its parts and territories, from an inaccessible Heart, within, without.\t|1| Is an expression |2| of the love animating all creation, |3| the simple joy that moves all from rock to life to mind to All. |4| It comes through this glorious One in all its parts and territories, from an inaccessible Heart, within, without.\n",
    "Privacy\tthe human form consicousness separates from the One even for a moment.\t|1| the human form |2| consicousness separates from the One even for a moment.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data\n",
    "\n",
    "##### Metrics while retaining word case and punctuations . and ' gives:\n",
    "\n",
    "* Subset removed Accuracy in terms of clause count only =  0.4489795918367347\n",
    "\n",
    "##### Metrics when case is lowered and punctuations . and  ,':\n",
    "* Subset removed Accuracy in terms of clause count only =  0.46938775510204084\n",
    "\n",
    "##### Metrics when case is lowered and no punctuations gives:\n",
    "* Subset removed Accuracy in terms of clause count only =  0.3877551020408163\n",
    "\n",
    "##### Metrics when case is retained and punctuations are removed gives:\n",
    "* Subset removed Accuracy in terms of clause count only =  0.3877551020408163\n",
    "\n",
    "##### Lower case and fullstop only\n",
    "* Subset removed Accuracy in terms of clause count only =  0.4489795918367347\n",
    "\n",
    "##### Lower case  + . , '  + verbs separated :\n",
    "* Subset removed Accuracy in terms of clause count only =  0.5918367346938775\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['my mom would come home more often.'], ['has a cat...', \"I don't have a cat\", 'though I have 2 dogs'], ['my hands get shaky'], ['my own dreams, nightmares', 'I have thousands of them'], ['its crazy and', 'i wish', 'i could keep up']]\n",
      "49 2\n",
      "Input Array Length 49\n"
     ]
    }
   ],
   "source": [
    "PATTERN = \"[^a-zA-Z0-9_\\s\\.',]+\"\n",
    "rgx = re.compile(PATTERN, re.IGNORECASE)\n",
    "\n",
    "arr = [line.split(\"\\t\")[:-1] for line in tsv.split(\"\\n\")]\n",
    "arr = [[y.lower() for y in x ] for x in arr] \n",
    "expected_clause_splitup = [line.split(\"\\t\")[-1] for line in tsv.split(\"\\n\")]\n",
    "expected_clause_splitup = [re.sub(\"\\|\\d+\\|\", \",,,\", x).split(\",,,\")[1:] for x in expected_clause_splitup]\n",
    "expected_clause_splitup = [[rgx.sub(' ', word) for word in clause] for clause in expected_clause_splitup]\n",
    "expected_clause_splitup = [[re.sub(\"\\s+\", ' ', word.strip()) for word in clause] for clause in expected_clause_splitup]\n",
    "\n",
    "print(expected_clause_splitup[:5])\n",
    "\n",
    "print(len(arr), len(arr[0]))\n",
    "ips = []\n",
    "for tok1, tok2 in arr:\n",
    "    original_ip = \"{} {}\".format(tok1, tok2)\n",
    "    ip = html.unescape(original_ip)\n",
    "    ip = rgx.sub(' ', ip)\n",
    "    ip = re.sub('\\s+', ' ', ip)\n",
    "    ips.append(ip)\n",
    "    \n",
    "print(\"Input Array Length\", len(ips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def print_sample_list(arr):\n",
    "    indices = [i for i in range(len(arr))]\n",
    "    shuffle(indices)\n",
    "    for x in indices[:min(5, len(arr))]:\n",
    "        print(x, arr[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the split clauses based on custom rules\n",
    "* For each clause, check if the tokens are in the prompt to remove them appropriately\n",
    "* The output of this module should be an array of array of strings for voice identification and scoring.\n",
    "\n",
    "To build the clauses, get the subtrees of all verbs. For each child token, get their children recursively:\n",
    "* IF the word is not a pronoun.\n",
    "* If the child is not an [X]comp ( acomp, xcomp, ccomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Pass Tokenization complete. Sample outputs are: \n",
      "39 [[my, main, problem, integrating, emptiness, with, everyday, life], [finding, meaning, in, meaningless], [given, the, meaningless, in], [meaning, making]]\n",
      "42 [[being, with, other, people], [mutually, mutating]]\n",
      "47 [[if, i, were, in, charge], [animating, all, creation], [that, moves, all, from, rock, to, life, to, to, all], [mind], [it, comes, through, this, glorious, one, in, all, its, parts, and, territories, from, an, inaccessible, heart, within, without]]\n",
      "44 [[calling, us, forth]]\n",
      "30 [[being, with, other, people], [be, an, isolating, or, a, experience, the, latter], [hugely, enriching], [when, each, others, presence, liberates, emergent, thoughts, or, self, expressions], [bringing, forth, a, collective]]\n"
     ]
    }
   ],
   "source": [
    "COMPOSITE_NOUN_ATTRIBUTES_ARR = [\"NOUN\", \"ADP\", \"ADJ\", \"PROPN\",  \"ADV\", \"CCONJ\", \"PART\"] \n",
    "COMPOSITE_NOUN_ATTRIBUTES_DICT = {\"VERB\" : [\"aux\", \"neg\"] }\n",
    "IGNORABLE_NOUN_DEPENDENCIES = [\"acl\"] #, \"advcl\"] #, \"relcl\"]\n",
    "\n",
    "def is_required_noun_child(word):\n",
    "    if word.pos_ in COMPOSITE_NOUN_ATTRIBUTES_ARR:\n",
    "        return True\n",
    "    if word.pos_ in COMPOSITE_NOUN_ATTRIBUTES_DICT.keys():\n",
    "        if word.dep_ in COMPOSITE_NOUN_ATTRIBUTES_DICT[word.pos_]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def flatten_list(l):\n",
    "    flat_list = [item for sublist in l for item in sublist]\n",
    "    #print(flat_list)\n",
    "    return flat_list\n",
    "\n",
    "def get_substrings(doc, word):\n",
    "    op = []\n",
    "    if word.pos_ == \"PUNCT\" or word.pos_ == \"VERB\": \n",
    "        return []\n",
    "    if not is_required_noun_child(word): \n",
    "        return [word]\n",
    "    \n",
    "    for l in word.lefts:\n",
    "        if l.dep_ not in IGNORABLE_NOUN_DEPENDENCIES:\n",
    "            op.append(get_substrings(doc, l))\n",
    "    op.append([word])\n",
    "    for r in word.rights:\n",
    "        if r.dep_ not in IGNORABLE_NOUN_DEPENDENCIES:\n",
    "            op.append(get_substrings(doc, r))\n",
    "    op = flatten_list(op)\n",
    "    return op\n",
    "\n",
    "IGNORABLE_VERB_DEPENDENCIES_ARR = [ \"advcl\"] #, \"conj\"]\n",
    "IGNORABLE_VERB_DEPENDENCIES_DICT = {\"ccomp\" : [\"VERB\"], \"conj\" : [\"VERB\"]}\n",
    "AUX_VERBS = [\"am\", \"was\", \"are\", \"is\", \"could\", \"would\", \"do\", \"has\", \"have\", \"can\", \"will\", \"did\", \"ca\", \"wo\"]\n",
    "\n",
    "def is_ignorable_verb_child(word):\n",
    "    if word.dep_ in IGNORABLE_VERB_DEPENDENCIES_ARR:\n",
    "        return True\n",
    "    if word.dep_ in IGNORABLE_VERB_DEPENDENCIES_DICT.keys():\n",
    "        if word.pos_ in IGNORABLE_VERB_DEPENDENCIES_DICT[word.dep_]: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_clauses(doc):\n",
    "    op = []\n",
    "    for word in doc:\n",
    "        if word.pos_ == \"VERB\" and word.text not in AUX_VERBS:\n",
    "            clause_arr = []\n",
    "            for l in word.lefts:\n",
    "                if not is_ignorable_verb_child(l):\n",
    "                    clause_arr.append(get_substrings(doc, l))\n",
    "\n",
    "            clause_arr.append([word]) #.text) #TODO: Recurse ??\n",
    "            \n",
    "            for r in word.rights:\n",
    "                if not is_ignorable_verb_child(r):\n",
    "                    clause_arr.append(get_substrings(doc, r))\n",
    "                    \n",
    "            if len([ignore for ignore in word.children]) == 0 and word.text in AUX_VERBS: #len(clause_arr) == 1 and clause_arr[0].pos_ == \"VERB\": #clause_arr[0] in AUX_VERBS:\n",
    "                continue\n",
    "            clause_arr = flatten_list(clause_arr)\n",
    "            op.append( clause_arr)#op.append(\" \".join(clause_arr))\n",
    "    return op\n",
    "\n",
    "def split_text(doc):\n",
    "    op = []\n",
    "    for sent in doc.sents:\n",
    "        tm = split_clauses(sent)\n",
    "        op = op + tm\n",
    "    return op\n",
    "\n",
    "tokenizer_output = []\n",
    "for i in range(len(ips)):\n",
    "    ip = ips[i]\n",
    "    doc = nlp(ip)\n",
    "    #print(ip, expected_clause_splitup[i])\n",
    "    clauses = split_text(doc)\n",
    "    tokenizer_output.append(clauses)\n",
    "print(\"First Pass Tokenization complete. Sample outputs are: \")\n",
    "print_sample_list(tokenizer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second pass to segregate possessive nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sometimes, i, wish]\n",
      "[sometimes, i, wish] False []\n",
      "[that, my, mom, come, home, more, often]\n",
      "[that, my, mom, come, home, more, often] True [my, mom]\n",
      "[my, hands, get, shaky]\n",
      "[my, hands, get, shaky] False []\n",
      "[at, times, i, worry, about, my, own, dreams]\n",
      "[at, times, i, worry, about, my, own, dreams] True [my, own, dreams]\n",
      "[it, 's, crazy]\n",
      "[it, 's, crazy] False []\n",
      "[i, wish]\n",
      "[i, wish] False []\n",
      "[i, keep, up]\n",
      "[i, keep, up] False []\n",
      "[when, i, get, mad]\n",
      "[when, i, get, mad] False []\n",
      "[i, sometimes, use, big, voice]\n",
      "[i, sometimes, use, big, voice] False []\n",
      "[then, i, go, somewhere, quiet]\n",
      "[then, i, go, somewhere, quiet] False []\n",
      "[so, i, cry, by, myself]\n",
      "[so, i, cry, by, myself] False []\n",
      "[that, i, not, entitled]\n",
      "[that, i, not, entitled] False []\n",
      "[i, not, be]\n",
      "[i, not, be] False []\n",
      "[when, i, get, mad]\n",
      "[when, i, get, mad] False []\n",
      "[it, takes, a, long, time, but]\n",
      "[it, takes, a, long, time, but] False []\n",
      "[to, build, up]\n",
      "[to, build, up] False []\n",
      "[to, forgive]\n",
      "[to, forgive] False []\n",
      "[when, they, avoided, me]\n",
      "[when, they, avoided, me] False []\n",
      "[i, felt, safe]\n",
      "[i, felt, safe] False []\n",
      "[knowing]\n",
      "[knowing] False []\n",
      "[that, i, receive, a, warm, welcome, from, my, family, and, friends]\n",
      "[that, i, receive, a, warm, welcome, from, my, family, and, friends] True [my, family]\n",
      "[sometimes, i, wish]\n",
      "[sometimes, i, wish] False []\n",
      "[that, i, be, younger, and]\n",
      "[that, i, be, younger, and] False []\n",
      "[done, things, differently]\n",
      "[done, things, differently] False []\n",
      "[when, they, avoided, me]\n",
      "[when, they, avoided, me] False []\n",
      "[i, sought]\n",
      "[i, sought] False []\n",
      "[to, find, out, why]\n",
      "[to, find, out, why] False []\n",
      "[knowing, the]\n",
      "[knowing, the] False []\n",
      "[why, begin, awareness, and, healing]\n",
      "[why, begin, awareness, and, healing] False []\n",
      "[being, with, other, people]\n",
      "[being, with, other, people] False []\n",
      "[to, exchange, views]\n",
      "[to, exchange, views] False []\n",
      "[if, i, think]\n",
      "[if, i, think] False []\n",
      "[i, try]\n",
      "[i, try] False []\n",
      "[to, find, other, ways, of]\n",
      "[to, find, other, ways, of] False []\n",
      "[doing, thing]\n",
      "[doing, thing] False []\n",
      "[might]\n",
      "[might] False []\n",
      "[be]\n",
      "[be] False []\n",
      "[but, sometime, i, disappointed, and]\n",
      "[but, sometime, i, disappointed, and] False []\n",
      "[stop, or]\n",
      "[stop, or] False []\n",
      "[quit]\n",
      "[quit] False []\n",
      "[my, father, taught, me]\n",
      "[my, father, taught, me] False []\n",
      "[how, to, think, critically, and]\n",
      "[how, to, think, critically, and] False []\n",
      "[to, love, deeply]\n",
      "[to, love, deeply] False []\n",
      "[if, i, n't, get]\n",
      "[if, i, n't, get] False []\n",
      "[what, i, want]\n",
      "[what, i, want] False []\n",
      "[i, try]\n",
      "[i, try] False []\n",
      "[to, work, out, another, way, of]\n",
      "[to, work, out, another, way, of] False []\n",
      "[getting]\n",
      "[getting] False []\n",
      "[what, i, want]\n",
      "[what, i, want] False []\n",
      "[when, they, avoided, me, hmmm]\n",
      "[when, they, avoided, me, hmmm] False []\n",
      "[i, 'm, stumped]\n",
      "[i, 'm, stumped] False []\n",
      "[how, to, respond, to, this]\n",
      "[how, to, respond, to, this] False []\n",
      "[loving]\n",
      "[loving] False []\n",
      "[other, people, deepen, with, familiarity]\n",
      "[other, people, deepen, with, familiarity] False []\n",
      "[even, with, those, add, color, to, the, world, as, well, as, an, opportunity]\n",
      "[even, with, those, add, color, to, the, world, as, well, as, an, opportunity] False []\n",
      "[to, turn, inward]\n",
      "[to, turn, inward] False []\n",
      "[to, see]\n",
      "[to, see] False []\n",
      "[being]\n",
      "[being] False []\n",
      "[what, triggered]\n",
      "[what, triggered] False []\n",
      "[having]\n",
      "[having] False []\n",
      "[said, that]\n",
      "[said, that] False []\n",
      "[it, 's, difficult]\n",
      "[it, 's, difficult] False []\n",
      "[for, me, to, open, my, heart, and, trust]\n",
      "[for, me, to, open, my, heart, and, trust] True [my, heart]\n",
      "[i, inspired]\n",
      "[i, inspired] False []\n",
      "[to, reach, out, and]\n",
      "[to, reach, out, and] False []\n",
      "[be, of, service]\n",
      "[be, of, service] False []\n",
      "[when, you, head, in, the, right, direction, no, matter]\n",
      "[when, you, head, in, the, right, direction, no, matter] False []\n",
      "[you, never, get, in, touch, with, them]\n",
      "[you, never, get, in, touch, with, them] False []\n",
      "[slow]\n",
      "[slow] False []\n",
      "[how, fast, or, you, travel]\n",
      "[how, fast, or, you, travel] False []\n",
      "[bring, global, humanity, closer, together, in, ways]\n",
      "[bring, global, humanity, closer, together, in, ways] False []\n",
      "[never, before, imagined]\n",
      "[never, before, imagined] False []\n",
      "[if, it, were]\n",
      "[if, it, were] False []\n",
      "[to, disappear, tomorrow]\n",
      "[to, disappear, tomorrow] False []\n",
      "[but, i, 'm, of, the, generation]\n",
      "[but, i, 'm, of, the, generation] False []\n",
      "[that, be, able]\n",
      "[that, be, able] False []\n",
      "[to, survive]\n",
      "[to, survive] False []\n",
      "[making, a, workplace, environment, healthy, and, successful]\n",
      "[making, a, workplace, environment, healthy, and, successful] False []\n",
      "[my, main, problem, getting, over, decades]\n",
      "[my, main, problem, getting, over, decades] False []\n",
      "[spent]\n",
      "[spent] False []\n",
      "[proving, myself]\n",
      "[proving, myself] False []\n",
      "[i, feel, sorry, for, the, pain]\n",
      "[i, feel, sorry, for, the, pain] False []\n",
      "[must]\n",
      "[must] False []\n",
      "[that, all, humans, and, all, beings, on, this, earth, endure, as, part, of, the, experience, of]\n",
      "[that, all, humans, and, all, beings, on, this, earth, endure, as, part, of, the, experience, of] False []\n",
      "[being, here]\n",
      "[being, here] False []\n",
      "[if, i, were, in, charge]\n",
      "[if, i, were, in, charge] False []\n",
      "[i, change, that, by, but, always, in, charge, of, their, interior, and, their, right]\n",
      "[i, change, that, by, but, always, in, charge, of, their, interior, and, their, right] True [their, interior]\n",
      "[facilitating, change]\n",
      "[facilitating, change] False []\n",
      "[that, made, everyone, in, charge, to, the, degree, of, their, capabilities, in, wordly, matters]\n",
      "[that, made, everyone, in, charge, to, the, degree, of, their, capabilities, in, wordly, matters] True [their, capabilities]\n",
      "[to, decide, for, themselves]\n",
      "[to, decide, for, themselves] False []\n",
      "[at, times, i, worry, about, big, big, existential, questions, and, little, microscopic, ones, sometimes, within, milliseconds, of, one]\n",
      "[at, times, i, worry, about, big, big, existential, questions, and, little, microscopic, ones, sometimes, within, milliseconds, of, one] False []\n",
      "[raising, a, family]\n",
      "[raising, a, family] False []\n",
      "[i, tending, the, future]\n",
      "[i, tending, the, future] False []\n",
      "[i, tending, the, life, processes]\n",
      "[i, tending, the, life, processes] False []\n",
      "[for, love, and, life, and, evolution, to, soar]\n",
      "[for, love, and, life, and, evolution, to, soar] False []\n",
      "[at, times, i, worry, about, and]\n",
      "[at, times, i, worry, about, and] False []\n",
      "[i, see, and]\n",
      "[i, see, and] False []\n",
      "[how, differently, process, things, from, many, of, the, people]\n",
      "[how, differently, process, things, from, many, of, the, people] False []\n",
      "[i, encounter]\n",
      "[i, encounter] False []\n",
      "[whether, i, wisely, acting, and]\n",
      "[whether, i, wisely, acting, and] False []\n",
      "[interacting]\n",
      "[interacting] False []\n",
      "[i, like]\n",
      "[i, like] False []\n",
      "[be, a, conduit]\n",
      "[be, a, conduit] False []\n",
      "[being, with, other, people]\n",
      "[being, with, other, people] False []\n",
      "[be, an, isolating, or, a, experience, the, latter]\n",
      "[be, an, isolating, or, a, experience, the, latter] False []\n",
      "[hugely, enriching]\n",
      "[hugely, enriching] False []\n",
      "[when, each, others, presence, liberates, emergent, thoughts, or, self, expressions]\n",
      "[when, each, others, presence, liberates, emergent, thoughts, or, self, expressions] False []\n",
      "[bringing, forth, a, collective]\n",
      "[bringing, forth, a, collective] False []\n",
      "[i, n't, mean]\n",
      "[i, n't, mean] False []\n",
      "[to, be, oppositional]\n",
      "[to, be, oppositional] False []\n",
      "[i, really, answer, outside, of, context]\n",
      "[i, really, answer, outside, of, context] False []\n",
      "[i, 'm, sorry]\n",
      "[i, 'm, sorry] False []\n",
      "[i, 'm, out, of, time]\n",
      "[i, 'm, out, of, time] False []\n",
      "[otherwise, i, write, a, bit, about, the, possibility, of, rights]\n",
      "[otherwise, i, write, a, bit, about, the, possibility, of, rights] False []\n",
      "[having, a, right]\n",
      "[having, a, right] False []\n",
      "[being, with, other, people]\n",
      "[being, with, other, people] False []\n",
      "[seeing, a, completely, different, version, of, self]\n",
      "[seeing, a, completely, different, version, of, self] False []\n",
      "[to, give, form, and, structure, to, the, present, moment]\n",
      "[to, give, form, and, structure, to, the, present, moment] False []\n",
      "[we, make, the, world, a, better, place]\n",
      "[we, make, the, world, a, better, place] False []\n",
      "[to, be, as, a, question]\n",
      "[to, be, as, a, question] False []\n",
      "[to, say]\n",
      "[to, say] False []\n",
      "[to, be, a, question]\n",
      "[to, be, a, question] False []\n",
      "[the, answer, be, a, question, and]\n",
      "[the, answer, be, a, question, and] False []\n",
      "[be]\n",
      "[be] False []\n",
      "[the, answers, and, questions, held, together]\n",
      "[the, answers, and, questions, held, together] False []\n",
      "[because, they, need, each, other, and]\n",
      "[because, they, need, each, other, and] False []\n",
      "[we, realize, a, little, faster]\n",
      "[we, realize, a, little, faster] False []\n",
      "[that, they, n't, end]\n",
      "[that, they, n't, end] False []\n",
      "[and, they, growing, together, and]\n",
      "[and, they, growing, together, and] False []\n",
      "[rest, in, that]\n",
      "[rest, in, that] False []\n",
      "[that, brings, light, and, eternal, love, without]\n",
      "[that, brings, light, and, eternal, love, without] False []\n",
      "[demanding, anything, of, the, others]\n",
      "[demanding, anything, of, the, others] False []\n",
      "[that, all, warmed, and]\n",
      "[that, all, warmed, and] False []\n",
      "[captivated, by, their, glow]\n",
      "[captivated, by, their, glow] True [their, glow]\n",
      "[one, an, energetic, and, gravitational, field, of, such, calm, peaceful, magnitude, brings, a, new, burst, life, to, those]\n",
      "[one, an, energetic, and, gravitational, field, of, such, calm, peaceful, magnitude, brings, a, new, burst, life, to, those] False []\n",
      "[my, conscience, bothers, me]\n",
      "[my, conscience, bothers, me] False []\n",
      "['m]\n",
      "['m] False []\n",
      "[if, i, not, placing, my, feet, on, the, path, with, an, open, heart, mind, and, spirit]\n",
      "[if, i, not, placing, my, feet, on, the, path, with, an, open, heart, mind, and, spirit] True [my, feet]\n",
      "[tuning, in, to]\n",
      "[tuning, in, to] False []\n",
      "['s]\n",
      "['s] False []\n",
      "[what, moving, and]\n",
      "[what, moving, and] False []\n",
      "[showing, up, fully, at, the, dance]\n",
      "[showing, up, fully, at, the, dance] False []\n",
      "[when, i, criticized]\n",
      "[when, i, criticized] False []\n",
      "[i, embody, my, personality, and]\n",
      "[i, embody, my, personality, and] True [my, personality]\n",
      "[feel, a, pang, contrast, to, the, story]\n",
      "[feel, a, pang, contrast, to, the, story] False []\n",
      "[i, tell, myself, inside, the, story]\n",
      "[i, tell, myself, inside, the, story] False []\n",
      "[we, tell, ourselves, of]\n",
      "[we, tell, ourselves, of] False []\n",
      "[what, we, think, in, the, moment]\n",
      "[what, we, think, in, the, moment] False []\n",
      "[to, be, an, absolute]\n",
      "[to, be, an, absolute] False []\n",
      "[before, it, falls, away, into, a, deeper, clearer, calm, of]\n",
      "[before, it, falls, away, into, a, deeper, clearer, calm, of] False []\n",
      "[letting]\n",
      "[letting] False []\n",
      "[go]\n",
      "[go] False []\n",
      "[i, like, about, myself]\n",
      "[i, like, about, myself] False []\n",
      "[that, here, i, sit, in, the, drop, of, this, essential, moment]\n",
      "[that, here, i, sit, in, the, drop, of, this, essential, moment] False []\n",
      "[chuckling, at, this, question]\n",
      "[chuckling, at, this, question] False []\n",
      "[my, main, problem, integrating, emptiness, with, everyday, life]\n",
      "[my, main, problem, integrating, emptiness, with, everyday, life] False []\n",
      "[finding, meaning, in, meaningless]\n",
      "[finding, meaning, in, meaningless] False []\n",
      "[given, the, meaningless, in]\n",
      "[given, the, meaningless, in] False []\n",
      "[meaning, making]\n",
      "[meaning, making] False []\n",
      "[how, my, narccissim, gets, in, the, way, of]\n",
      "[how, my, narccissim, gets, in, the, way, of] True [my, narccissim]\n",
      "[seeing, the, truth, of, the, illusion, of, self]\n",
      "[seeing, the, truth, of, the, illusion, of, self] False []\n",
      "[manifesting, through, each, emanation]\n",
      "[manifesting, through, each, emanation] False []\n",
      "[sparkling]\n",
      "[sparkling] False []\n",
      "[called, me]\n",
      "[called, me] False []\n",
      "[being, with, other, people]\n",
      "[being, with, other, people] False []\n",
      "[mutually, mutating]\n",
      "[mutually, mutating] False []\n",
      "[i, like]\n",
      "[i, like] False []\n",
      "[lay, back, and]\n",
      "[lay, back, and] False []\n",
      "[live, the, window]\n",
      "[live, the, window] False []\n",
      "[i, become, that, as, if, a, home, with, no, walls]\n",
      "[i, become, that, as, if, a, home, with, no, walls] False []\n",
      "[the, whole, field, exists]\n",
      "[the, whole, field, exists] False []\n",
      "[calling, us, forth]\n",
      "[calling, us, forth] False []\n",
      "[when, i, get, mad]\n",
      "[when, i, get, mad] False []\n",
      "[i, feel, adrenaline, through, my, human, body, and]\n",
      "[i, feel, adrenaline, through, my, human, body, and] True [my, human, body]\n",
      "[and, i, love, it]\n",
      "[and, i, love, it] False []\n",
      "[because, it, moves, me]\n",
      "[because, it, moves, me] False []\n",
      "[which, moves, all]\n",
      "[which, moves, all] False []\n",
      "[raising, a, family]\n",
      "[raising, a, family] False []\n",
      "[captivates, my, whole, being]\n",
      "[captivates, my, whole, being] True [my, whole, being]\n",
      "[just, as, every, window, on, reality, sees]\n",
      "[just, as, every, window, on, reality, sees] False []\n",
      "[it, held, in, love, beyond]\n",
      "[it, held, in, love, beyond] False []\n",
      "[knowing]\n",
      "[knowing] False []\n",
      "[if, i, were, in, charge]\n",
      "[if, i, were, in, charge] False []\n",
      "[animating, all, creation]\n",
      "[animating, all, creation] False []\n",
      "[that, moves, all, from, rock, to, life, to, to, all]\n",
      "[that, moves, all, from, rock, to, life, to, to, all] False []\n",
      "[mind]\n",
      "[mind] False []\n",
      "[it, comes, through, this, glorious, one, in, all, its, parts, and, territories, from, an, inaccessible, heart, within, without]\n",
      "[it, comes, through, this, glorious, one, in, all, its, parts, and, territories, from, an, inaccessible, heart, within, without] True [its, parts]\n",
      "[privacy, the, human, form, consicousness, separates, from, the, one, even, for, a, moment]\n",
      "[privacy, the, human, form, consicousness, separates, from, the, one, even, for, a, moment] False []\n",
      "Possessive nouns extracted. Sample outputs:\n",
      "9 [[when, they, avoided, me], [i, felt, safe], [knowing], [that, i, receive, a, warm, welcome, from, my, family, and, friends]]\n",
      "33 [[to, give, form, and, structure, to, the, present, moment]]\n",
      "22 [[making, a, workplace, environment, healthy, and, successful]]\n",
      "15 [[my, father, taught, me], [how, to, think, critically, and], [to, love, deeply]]\n",
      "21 [[bring, global, humanity, closer, together, in, ways], [never, before, imagined], [if, it, were], [to, disappear, tomorrow], [but, i, 'm, of, the, generation], [that, be, able], [to, survive]]\n"
     ]
    }
   ],
   "source": [
    "def split_possessive_phrases(tokens_arr):\n",
    "    op_arr = []\n",
    "    is_splittable = False\n",
    "    for i in range(len(tokens_arr)):\n",
    "        token = tokens_arr[i]\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            children =  [x for x in token.children]\n",
    "            end = i\n",
    "            start = None\n",
    "            for child in children:\n",
    "                if child.dep_ == \"poss\":\n",
    "                    start = tokens_arr.index(child)\n",
    "                    break\n",
    "            if start:\n",
    "                op_arr =  tokens_arr[start:end+1]\n",
    "                is_splittable = True\n",
    "                break\n",
    "    print(tokens_arr, is_splittable, op_arr)\n",
    "    return is_splittable, op_arr\n",
    "           \n",
    "tokenizer_output_pass2 = []\n",
    "for i in range(len(tokenizer_output)):\n",
    "    sent = tokenizer_output[i]\n",
    "    new_clauses = []\n",
    "    for clauses in sent:\n",
    "        print(clauses)\n",
    "        is_splittable, new_arr = split_possessive_phrases(clauses)\n",
    "        new_clauses.append(clauses)\n",
    "    tokenizer_output_pass2.append(new_clauses)\n",
    "    \n",
    "print(\"Possessive nouns extracted. Sample outputs:\")\n",
    "print_sample_list(tokenizer_output_pass2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: 3D array of spacy tokens per clause per input string\n",
    "Output: 2D Array of clauses per input string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.token.Token' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9e408ae59f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_clauses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclauses\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclauses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnew_clauses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object is not iterable"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "for i in range(len(tokenizer_output_pass2)):\n",
    "    sent = tokenizer_output_pass2[i]\n",
    "    prompt_doc = nlp(arr[i][0])\n",
    "    ignore_indices = [x.i for x in prompt_doc]\n",
    "    new_clauses = []\n",
    "    for clauses in sent:\n",
    "        op = ' '.join([x.text for x in clauses if x.i not in ignore_indices]).strip()\n",
    "        if op != '':\n",
    "            new_clauses.append(op)\n",
    "    tokenized_sentences.append(new_clauses)\n",
    "\n",
    "print(\"Text extracted with prompt tokens removed. Sample outputs:\")\n",
    "print_sample_list(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clauses cleanup:\n",
    "\n",
    "1. Remove clauses which are subsets of another. (This step is not required in larger corpora)\n",
    "2. Replace tokens that are spacy relevant (do n't with don't and it 's with it's)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First pass Accuracy in terms of clause count only = \", sum([len(expected_clause_splitup[i])==len(tokenized_sentences[i]) for i in range(len(tokenized_sentences))])/len(tokenized_sentences))\n",
    "tokenized_sentences = [[ clause.replace(\" n't\", \"n't\").replace(\" 'm\", \"'m\").replace(\" 's\", \"'s\") for clause in clauses] for clauses in tokenized_sentences]\n",
    "print(\"Spacy tokens replaced Accuracy in terms of clause count only = \", sum([len(expected_clause_splitup[i])==len(tokenized_sentences[i]) for i in range(len(tokenized_sentences))])/len(tokenized_sentences))\n",
    "\n",
    "\n",
    "cleaned_sentences = [ [clause.strip() for clause in sent if clause.strip() != ''] for sent in tokenized_sentences]\n",
    "print(\"Prompt removed Accuracy in terms of clause count only = \", sum([len(expected_clause_splitup[i])==len(cleaned_sentences[i]) for i in range(len(cleaned_sentences))])/len(cleaned_sentences))\n",
    "\n",
    "final_sentences = []\n",
    "for i in range(len(cleaned_sentences)):\n",
    "    old_clauses = cleaned_sentences[i]\n",
    "    is_subset = [True in [c != clause and c.find(clause)>=0 for c in old_clauses] for clause in old_clauses]\n",
    "    new_clauses = [ old_clauses[j] for j in range(len(old_clauses)) if not is_subset[j]]\n",
    "    final_sentences.append(new_clauses)\n",
    "    \n",
    "print(\"Subset removed Accuracy in terms of clause count only = \", sum([len(expected_clause_splitup[i])==len(final_sentences[i]) for i in range(len(final_sentences))])/len(final_sentences))\n",
    "\n",
    "for i in range(len(ips)):\n",
    "    if len(expected_clause_splitup[i]) != len(final_sentences[i]):\n",
    "        print(\"######################################################################\")\n",
    "        print(i, len(expected_clause_splitup[i]), len(cleaned_sentences[i]), len(final_sentences[i]))\n",
    "        print(expected_clause_splitup[i])\n",
    "        print(cleaned_sentences[i])\n",
    "        print(final_sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse Tree with predictions as a HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ips)):\n",
    "    ip = ips[i]\n",
    "    doc = nlp(ip)\n",
    "    a = displacy.render(doc, style='dep') # Saved in html folder\n",
    "\n",
    "    html_str_arr = [\"<html><body>\"]\n",
    "    html_str_arr.append(\"<div>Input: {}</div>\".format(ips[i]))\n",
    "    html_str_arr.append(\"<div>Expected Output: {}</div>\".format(expected_clause_splitup[i]))\n",
    "    html_str_arr.append(\"<div>Predicted Output: {}</div>\".format(final_sentences[i]))\n",
    "    html_str_arr.append(\"<div>{}</div></body></html>\".format(a))\n",
    "    \n",
    "    with open(\"html/file_{}.html\".format(i), \"w\") as f:\n",
    "        f.writelines(html_str_arr)\n",
    "print(\"HTML files written into html folder\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
